# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oZjivZCITuAouygEIB1fpjgwy0H79j5I
"""

# installing packages
!pip install tensorflow

# loading data from kaggle
from google.colab import files
files.upload() #upload kaggle.json with login and key

!pip install -q kaggle

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 /root/.kaggle/kaggle.json

!kaggle competitions download -c digit-recognizer --force

!mkdir digit-recognizer
!unzip digit-recognizer.zip -d digit-recognizer

# importing libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Input, Dense, Conv1D, Conv2D, Flatten, Dense, AvgPool1D, AvgPool2D, MaxPooling2D, GlobalAveragePooling2D

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, RepeatedKFold, cross_val_score, RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score

from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.utils import to_categorical

import warnings
warnings.filterwarnings('ignore')

"""## Data Analysis and preprocessing"""

train = pd.read_csv("/content/digit-recognizer/train.csv")
test = pd.read_csv("/content/digit-recognizer/test.csv")

# function to preview the dataframe
def data_preview(data):
    name =[x for x in globals() if globals()[x] is data][0]
    print('Информация о таблице', name, ':')
    display(data.info())
    with pd.option_context('display.max_columns', None):
        display(data.head(5))  
    print(data.isna().sum().sort_values(ascending=False).head(20))
    print(data.duplicated().sum())

data_preview(train)

data_preview(test)

# plotting the distribution of labels
train['label'].value_counts().sort_values().plot(kind='bar')

# displaying shares of different labels
for i in range(0,10):
    label_percentage = len(train[train['label'] == i]) / len(train['label']) 
    print(f'A share of {i} label is {label_percentage:.1%}')

"""The dataset includes 42000 entries. Each entry contains a series of numbers in range [0,255] for each pixel of a picture, and a label containing a number 0-9 written in the picture. 
The proportion of each label value is approximately 10% of the total dataset.
There are no missing values or duplicates in the dataset. 
"""

# defining features and a target
y_train_all = train["label"].values
x_train_all = train.drop("label", axis=1)
x_train_visualize = x_train_all.values.reshape(-1, 28, 28)

# plotting a sample of numbers and labels
plt.figure(figsize=(12, 5), tight_layout=True)

for idx, digit in enumerate(x_train_visualize[:10]):
    plt.subplot(2, 5, idx + 1)
    plt.title(y_train_all[idx])
    plt.imshow(digit)

plt.show()

# normalization
x_train_all = x_train_all / 255.0

# splitting the dataset
x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=12345, stratify=y_train_all) # разделили выборки в пропорции 2 к 8

print(x_train.shape)
print(y_train.shape)
print(x_valid.shape)
print(y_valid.shape)

"""## Building neural networks

### Fully connected NN
"""

y_train_fcnn = to_categorical(y_train, 28)
y_valid_fcnn = to_categorical(y_valid, 28)

model_fcnn = keras.models.Sequential()

# adding dense layers
model_fcnn.add(keras.layers.Dense(units=100, 
                             activation='sigmoid'))
model_fcnn.add(keras.layers.Dense(units=28, 
                             activation='relu'))
model_fcnn.add(keras.layers.Dense(units=28, 
                             activation='softmax'))
model_fcnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
history_fcnn = model_fcnn.fit(x_train, y_train_fcnn, epochs=10, verbose=2, validation_data=(x_valid, y_valid_fcnn))
model_fcnn.summary()

plt.plot(history_fcnn.history['loss'], label="loss")
plt.plot(history_fcnn.history['val_loss'], label="val_loss")
plt.plot(history_fcnn.history['acc'], label="acc")
plt.plot(history_fcnn.history['val_acc'], label="val_acc")
plt.legend(loc="center right")

"""### Convolutional NN: LeNet"""

x_train_conv = x_train.values.reshape(-1, 28, 28)
x_valid_conv = x_valid.values.reshape(-1, 28, 28)

model_conv = Sequential()

model_conv.add(Conv2D(6, (3, 3), padding='same', activation='tanh',
                 input_shape=(28, 28, 1)))
model_conv.add(AvgPool2D(pool_size=(2, 2)))
model_conv.add(Conv2D(16, (3, 3), padding='valid', activation='tanh',
                 input_shape=(10, 10, 1)))
model_conv.add(AvgPool2D(pool_size=(2, 2)))
model_conv.add(Flatten())
model_conv.add(Dense(units=120, activation='tanh'))
model_conv.add(Dense(units=84, activation='tanh'))
model_conv.add(Dense(units=10, activation='softmax'))

model_conv.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])
model_conv.summary()
history_conv = model_conv.fit(x_train_conv, y_train, epochs=4, verbose=1,
          steps_per_epoch=x_train_conv.shape[0]//84, validation_data=(x_valid_conv, y_valid), validation_steps=x_valid_conv.shape[0]//84)

plt.plot(history_conv.history['loss'], label="loss")
plt.plot(history_conv.history['val_loss'], label="val_loss")
plt.plot(history_conv.history['acc'], label="acc")
plt.plot(history_conv.history['val_acc'], label="val_acc")
plt.legend(loc="center right")

# saving the best model
model_conv.save('model_conv.h5')

"""### Making predictions"""

test_conv = test.values.reshape(-1, 28, 28) / 255

# making predictions
results = model_conv.predict(test_conv, verbose=0).argmax(axis=1)

# saving predictions to csv
sumbission = pd.DataFrame({"ImageID": range(1, len(test_conv) + 1), "Label": results})
sumbission.set_index("ImageID").to_csv("submission.csv")

sumbission.head()
